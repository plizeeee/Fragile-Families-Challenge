Hyperparameter Tuning
![alt text](https://github.com/plizeeee/Fragile-Families-Challenge/blob/main/Images/Accuracies%20on%20Test%20Set.PNG)
### Interpretting Machine Learning Models
The best performing models were interpretted using Shapley Additive Explanation (SHAP) plots to understand which features were driving the model predictions for each life outcome (and better understand the factors associated with each life outcome). SHAP feature importance plots were used to understand the global importance of each feature for model predictions – To put it differently, whenever the model makes a prediction, which features, on average, affect the model’s prediction most? The features importance plots do not give any indication on the direction of the effect (positive or negative) or each feature on model prediction. Therefore, SHAP bee swarm plots were used to visualize the individual effects of each observed feature value in the dataset on the output to better understand if the features positively or negatively affect the predicted outcome.

Example plots are shown below for the GPA lifeoutcome, where it can be seen that the feature that had the greatest effect on model predictions was the child's performance on the Woodcock Johnson test at the age of 10. It can be seen from the beeswarm plots that the model tends to predict higher GPAs at the age of 16 for students that had a higher Woodcock Johnson test score at the age of 10. Many of these features agree with common sense, so the model appears to have learned relevant features from the 10000+ initial features. The same plots for the other six life outcomes can be found in my full report.
![alt text](https://github.com/plizeeee/Fragile-Families-Challenge/blob/main/Images/Example%20SHAP%20Features%20Importance%20Plot.PNG)
![alt text](https://github.com/plizeeee/Fragile-Families-Challenge/blob/main/Images/Example%20SHAP%20Bee%20Swarm%20Plot.PNG)